{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "from keras import layers\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import onnxruntime as ort\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./data/segmentation/images/\"\n",
    "target_dir = \"./data/segmentation/annotations/trimaps/\"\n",
    "img_size = (160, 160)\n",
    "num_classes = 3\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 7390\n",
      "./data/segmentation/images/Abyssinian_1.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_1.png\n",
      "./data/segmentation/images/Abyssinian_10.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_10.png\n",
      "./data/segmentation/images/Abyssinian_100.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_100.png\n",
      "./data/segmentation/images/Abyssinian_101.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_101.png\n",
      "./data/segmentation/images/Abyssinian_102.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_102.png\n",
      "./data/segmentation/images/Abyssinian_103.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_103.png\n",
      "./data/segmentation/images/Abyssinian_104.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_104.png\n",
      "./data/segmentation/images/Abyssinian_105.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_105.png\n",
      "./data/segmentation/images/Abyssinian_106.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_106.png\n",
      "./data/segmentation/images/Abyssinian_107.jpg | ./data/segmentation/annotations/trimaps/Abyssinian_107.png\n"
     ]
    }
   ],
   "source": [
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    max_dataset_len=None,\n",
    "):\n",
    "    # Metoda zwraca obraz oraz jego anotację, zwróć uwagę, że obrazki nie są skalowane do 0-1,\n",
    "    # będzie to mieć znaczenie przy używaniu sieci w Unity\n",
    "    def load_img_masks(input_img_path, target_img_path):\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.resize(input_img, img_size)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=1)\n",
    "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
    "\n",
    "        # Etykiety to 1, 2, 3. Odejmujemy jeden, aby otrzymać 0, 1, 2\n",
    "        target_img -= 1\n",
    "        return input_img, target_img\n",
    "\n",
    "    # Jesli chcemy możemy ograniczyć zbióra danych na potrzeby debugowania\n",
    "    if max_dataset_len:\n",
    "        input_img_paths = input_img_paths[:max_dataset_len]\n",
    "        target_img_paths = target_img_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "\n",
    "    # Downsampling\n",
    "\n",
    "    # Blok wejsciowy\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # residuum\n",
    "\n",
    "    # Bloki 1, 2, 3 mają taką samą strukturę, ale inną liczbę neuronów\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # dodanie residuum\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    # upsampling\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # dodanie residuum\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Warstwa wyjsciowa dokonująca klasyfikacji poszczególnych pikseli\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 80, 80, 32)   896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 80, 80, 32)  128         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 80, 80, 32)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 80, 80, 32)   0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 80, 80, 64)  2400        ['activation_1[0][0]']           \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 80, 80, 64)  256         ['separable_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 80, 80, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 80, 80, 64)  4736        ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 80, 80, 64)  256         ['separable_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 40, 40, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 40, 40, 64)   2112        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 40, 40, 64)   0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 40, 40, 64)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 40, 40, 128)  8896       ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 40, 40, 128)  512        ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 40, 40, 128)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 40, 40, 128)  17664      ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 40, 40, 128)  512        ['separable_conv2d_3[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 20, 20, 128)  0          ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 20, 20, 128)  8320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 20, 20, 128)  0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 20, 20, 128)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 20, 20, 256)  34176      ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 20, 20, 256)  1024       ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 20, 20, 256)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 20, 20, 256)  68096      ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 20, 20, 256)  1024       ['separable_conv2d_5[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 256)  0          ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 10, 10, 256)  33024       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 10, 10, 256)  0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 10, 10, 256)  0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 10, 10, 256)  590080     ['activation_7[0][0]']           \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 10, 10, 256)  1024       ['conv2d_transpose[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 10, 10, 256)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 10, 10, 256)  590080     ['activation_8[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 10, 10, 256)  1024       ['conv2d_transpose_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 20, 20, 256)  0          ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 20, 20, 256)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 20, 20, 256)  65792       ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 20, 20, 256)  0           ['up_sampling2d[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 20, 20, 256)  0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 20, 20, 128)  295040     ['activation_9[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 20, 20, 128)  512        ['conv2d_transpose_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 20, 20, 128)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 20, 20, 128)  147584     ['activation_10[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 20, 20, 128)  512        ['conv2d_transpose_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 40, 40, 256)  0          ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 40, 40, 128)  0          ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 40, 40, 128)  32896       ['up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 40, 40, 128)  0           ['up_sampling2d_2[0][0]',        \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 40, 40, 128)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 40, 40, 64)  73792       ['activation_11[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 40, 40, 64)  256         ['conv2d_transpose_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 40, 40, 64)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 40, 40, 64)  36928       ['activation_12[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 40, 40, 64)  256         ['conv2d_transpose_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSampling2D)  (None, 80, 80, 128)  0          ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSampling2D)  (None, 80, 80, 64)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 80, 80, 64)   8256        ['up_sampling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 80, 80, 64)   0           ['up_sampling2d_4[0][0]',        \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 80, 80, 64)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 80, 80, 32)  18464       ['activation_13[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 80, 80, 32)  128         ['conv2d_transpose_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 80, 80, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 80, 80, 32)  9248        ['activation_14[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 80, 80, 32)  128         ['conv2d_transpose_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSampling2D)  (None, 160, 160, 64  0          ['add_5[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSampling2D)  (None, 160, 160, 32  0          ['batch_normalization_14[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 160, 160, 32  2080        ['up_sampling2d_7[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 160, 160, 32  0           ['up_sampling2d_6[0][0]',        \n",
      "                                )                                 'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 160, 160, 3)  867         ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,058,979\n",
      "Trainable params: 2,055,203\n",
      "Non-trainable params: 3,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = 1000\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    train_input_img_paths,\n",
    "    train_target_img_paths,\n",
    "    max_dataset_len=1000,# usuń, jesli chcesz trenować na całym zbiorze danych\n",
    ")\n",
    "valid_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"unet_segmentation.keras\", save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 76s - loss: 0.6605 - val_loss: 0.6634 - 76s/epoch - 226ms/step\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 76s - loss: 0.5977 - val_loss: 0.6682 - 76s/epoch - 228ms/step\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 74s - loss: 0.5387 - val_loss: 0.7231 - 74s/epoch - 220ms/step\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 77s - loss: 0.4723 - val_loss: 0.7789 - 77s/epoch - 229ms/step\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 73s - loss: 0.3978 - val_loss: 0.9826 - 73s/epoch - 218ms/step\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 77s - loss: 0.3382 - val_loss: 1.5963 - 77s/epoch - 231ms/step\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 70s - loss: 0.3247 - val_loss: 1.1217 - 70s/epoch - 211ms/step\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 71s - loss: 0.3311 - val_loss: 1.3146 - 71s/epoch - 213ms/step\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 75s - loss: 0.2958 - val_loss: 0.8880 - 75s/epoch - 224ms/step\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 68s - loss: 0.2589 - val_loss: 0.9252 - 68s/epoch - 205ms/step\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 70s - loss: 0.2319 - val_loss: 1.0577 - 70s/epoch - 210ms/step\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 69s - loss: 0.2198 - val_loss: 0.9929 - 69s/epoch - 208ms/step\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 68s - loss: 0.2116 - val_loss: 1.0471 - 68s/epoch - 203ms/step\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 69s - loss: 0.1965 - val_loss: 0.9471 - 69s/epoch - 207ms/step\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 - 70s - loss: 0.1832 - val_loss: 1.0826 - 70s/epoch - 208ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x31411b7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 15\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 23). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: unet_segmentation_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: unet_segmentation_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "save_model_path=\"unet_segmentation_saved_model\"\n",
    "if not os.path.isdir(save_model_path):\n",
    "      os.makedirs(save_model_path)\n",
    "\n",
    "model.save(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eryk/Documents/Metody głębokiego uczenia w systemach wizyjnych i wirtualnej rzeczywistości/deep-learning-vr-uni-projects/project01/unet_segmentation_saved_model.zip'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(save_model_path, 'zip', save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eryk/miniconda3/envs/ml/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2025-03-26 19:01:15,230 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2025-03-26 19:01:16,488 - INFO - Signatures found in model: [serving_default].\n",
      "2025-03-26 19:01:16,489 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2025-03-26 19:01:16,489 - INFO - Output names: ['conv2d_8']\n",
      "2025-03-26 19:01:16,959 - INFO - Using tensorflow=2.12.0, onnx=1.17.0, tf2onnx=1.16.1/15c810\n",
      "2025-03-26 19:01:16,959 - INFO - Using opset <onnx, 11>\n",
      "2025-03-26 19:01:17,009 - INFO - Computed 0 values for constant folding\n",
      "2025-03-26 19:01:17,121 - INFO - Optimizing ONNX model\n",
      "2025-03-26 19:01:17,422 - INFO - After optimization: BatchNormalization -1 (15->14), Cast -8 (8->0), Concat -8 (8->0), Const -104 (214->110), Identity -2 (2->0), Reshape -6 (6->0), Shape -8 (8->0), Slice -8 (8->0), Squeeze -8 (8->0), Transpose -114 (116->2), Unsqueeze -32 (32->0)\n",
      "2025-03-26 19:01:17,436 - INFO - \n",
      "2025-03-26 19:01:17,436 - INFO - Successfully converted TensorFlow model unet_segmentation_saved_model to ONNX\n",
      "2025-03-26 19:01:17,436 - INFO - Model inputs: ['input_1']\n",
      "2025-03-26 19:01:17,436 - INFO - Model outputs: ['conv2d_8']\n",
      "2025-03-26 19:01:17,436 - INFO - ONNX model is saved at unet_segmentation_saved_model.onnx\n"
     ]
    }
   ],
   "source": [
    "# !unzip unet_segmentation_saved_model.zip -d unet_segmentation_saved_model\n",
    "!python -m tf2onnx.convert --saved-model unet_segmentation_saved_model --opset 11 --output unet_segmentation_saved_model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = 'unet_segmentation_saved_model.onnx'\n",
    "img_size = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./data/segmentation/images/Abyssinian_1.jpg\").convert('RGB').resize((160,160))\n",
    "input_img=np.expand_dims(np.asarray(img, dtype=\"float32\"),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mask(val_preds):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds, axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    img = np.array(img)\n",
    "    cv2.imshow('a', img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "def display_mask2(val_preds):\n",
    "    cv2.imshow('a', val_preds)\n",
    "    cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-03-26 19:02:16.779506 [W:onnxruntime:, coreml_execution_provider.cc:112 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 13 number of nodes in the graph: 73 number of nodes supported by CoreML: 56\u001b[m\n",
      "2025-03-26 19:02:17.745 python[51119:4631707] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-26 19:02:17.745 python[51119:4631707] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "sess_ort = ort.InferenceSession(onnx_path,providers=ort.get_available_providers())\n",
    "\n",
    "outputs = sess_ort.run(None, {sess_ort.get_inputs()[0].name: input_img})\n",
    "outputs = outputs[0]\n",
    "display_mask2(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
